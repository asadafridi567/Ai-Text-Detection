{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwJmglTl4NgC",
        "outputId": "468f6599-5f1b-499e-b72b-d9d223772c28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.4)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.4.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.7.9)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=5394e3b53682c4f1fd9dad2034430fd7759ab7328c1b32587d1f2656d36df0d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=6b165bcd738f5f26040e3f9afd3605edee3765140b763c054dbb9267f923dd15\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=21877c6ab953ac87c758ddf1abd898c6223f0f806782c5db14d592edc3196a6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=6932b2ef200df517e4fb6fe05774de09b7fa4121b35b067954c5d6dd9b35d42c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, python-dotenv, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, feedparser, cssselect, requests-file, nvidia-cusparse-cu12, nvidia-cudnn-cu12, feedfinder2, tldextract, nvidia-cusolver-cu12, flask-ngrok, newspaper3k\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.11 flask-ngrok-0.0.25 jieba3k-0.35.1 newspaper3k-0.2.8 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 python-dotenv-1.1.1 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install flask flask-ngrok sentence-transformers newspaper3k python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "comvYBMTp6o3",
        "outputId": "e334bf72-d378-4fa8-804d-1a31ac7c43f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0d1j-8r461W",
        "outputId": "249030f7-2395-4fbf-a806-f6a6c1d22e5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "69194c2d84d61f7bb4bc4e57c98ee761c48c8abd47e548551eba59f9c68e2b9f\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_SEARCH_API_KEY\")\n",
        "print(GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgHab4HiOPxJ",
        "outputId": "54c8b92d-0ed5-425d-dd55-10c817584dc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.12-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.12\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qhyDUUDL-j9",
        "outputId": "8549a87b-32b3-417f-a473-8e19001968f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Ngrok tunnel URL: NgrokTunnel: \"https://f3c240bbba8e.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [15/Jul/2025 15:42:18] \"GET /check/ HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Scraping: https://www.cnn.com/2025/07/12/us/timeline-texas-flooding\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [15/Jul/2025 15:42:30] \"POST /check/ HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ Skipping blocked domain: https://www.nytimes.com/live/2025/07/06/us/texas-floods\n",
            "ðŸ” Scraping: https://www.latimes.com/california/story/2025-07-09/california-texas-flooding\n",
            "ðŸ” Scraping: https://www.npr.org/2025/07/07/nx-s1-5459755/texas-floods-climate-change\n",
            "ðŸ” Scraping: https://www.newsweek.com/texas-flash-floods-deaths-missing-storms-live-updates-2095388\n",
            "ðŸ” Scraping: https://pmc.ncbi.nlm.nih.gov/articles/PMC9013542/\n",
            "âŒ Skipping blocked domain: https://www.sciencedirect.com/science/article/pii/S1470160X24004011\n",
            "ðŸ” Scraping: https://6abc.com/post/search-texas-flooding-victims-paused-heavy-rain-prompts-kerr-county-flash-flood-warning-guadalupe-river-corridor-evacuated/17101757/\n",
            "ðŸ” Scraping: https://www.nbcnews.com/news/us-news/texas-officials-defend-response-deadly-floods-saved-many-people-rcna218638\n",
            "ðŸ” Scraping: https://gov.texas.gov/news/post/governor-abbott-continues-state-response-recovery-efforts-for-texans-impacted-by-flooding\n",
            "ðŸ” Scraping: https://www.illinois.gov/news/press-release.30430.html\n",
            "ðŸ” Scraping: https://www.nationalguard.mil/News/Article-View/Article/3919179/national-guard-rescues-hundreds-of-people-in-wake-of-hurricane/\n",
            "âš ï¸ Error scraping https://www.nationalguard.mil/News/Article-View/Article/3919179/national-guard-rescues-hundreds-of-people-in-wake-of-hurricane/: 403 Client Error: Forbidden for url: https://www.nationalguard.mil/News/Article-View/Article/3919179/national-guard-rescues-hundreds-of-people-in-wake-of-hurricane/\n",
            "ðŸ” Scraping: https://ky.ng.mil/News/Article/4180232/national-guard-answers-the-call-as-near-historic-flooding-isolates-nelson-count/\n",
            "âš ï¸ Error scraping https://ky.ng.mil/News/Article/4180232/national-guard-answers-the-call-as-near-historic-flooding-isolates-nelson-count/: 403 Client Error: Forbidden for url: https://ky.ng.mil/News/Article/4180232/national-guard-answers-the-call-as-near-historic-flooding-isolates-nelson-count/\n",
            "ðŸ” Scraping: https://www.nationalguard.mil/News/Article-View/Article/740511/texas-guard-teams-up-with-other-agencies-to-rescue-more-than-200-from-floods/\n",
            "âš ï¸ Error scraping https://www.nationalguard.mil/News/Article-View/Article/740511/texas-guard-teams-up-with-other-agencies-to-rescue-more-than-200-from-floods/: 403 Client Error: Forbidden for url: https://www.nationalguard.mil/News/Article-View/Article/740511/texas-guard-teams-up-with-other-agencies-to-rescue-more-than-200-from-floods/\n",
            "âŒ Skipping blocked domain: https://www.nytimes.com/2025/07/08/climate/why-the-texas-floods-were-so-deadly.html\n",
            "ðŸ” Scraping: https://apnews.com/article/texas-floods-extreme-weather-attitudes-preparations-cc9d55c1f2440d78e01dcc65ec748112\n",
            "ðŸ” Scraping: https://spectrumlocalnews.com/tx/south-texas-el-paso/politics/2025/07/14/environmentalists--climate-change-played-role-in-flooding\n",
            "ðŸ” Scraping: https://news.yahoo.com/nws-expands-ny-flood-warning-224713104.html\n",
            "ðŸ” Scraping: https://www.cnn.com/weather/live-news/texas-flooding-camp-mystic-07-06-25-hnk\n",
            "ðŸ” Scraping: https://news.yahoo.com/village-ruidoso-responds-concerns-raised-054119224.html\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [15/Jul/2025 15:45:34] \"POST /check/ HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ Skipping blocked domain: https://www.nytimes.com/live/2025/07/06/us/texas-floods\n",
            "ðŸ” Scraping: https://www.latimes.com/california/story/2025-07-09/california-texas-flooding\n",
            "ðŸ” Scraping: https://www.npr.org/2025/07/07/nx-s1-5459755/texas-floods-climate-change\n",
            "ðŸ” Scraping: https://www.newsweek.com/texas-flash-floods-deaths-missing-storms-live-updates-2095388\n",
            "ðŸ” Scraping: https://pmc.ncbi.nlm.nih.gov/articles/PMC9013542/\n",
            "âŒ Skipping blocked domain: https://www.sciencedirect.com/science/article/pii/S1470160X24004011\n",
            "ðŸ” Scraping: https://6abc.com/post/search-texas-flooding-victims-paused-heavy-rain-prompts-kerr-county-flash-flood-warning-guadalupe-river-corridor-evacuated/17101757/\n",
            "ðŸ” Scraping: https://www.nbcnews.com/news/us-news/texas-officials-defend-response-deadly-floods-saved-many-people-rcna218638\n",
            "ðŸ” Scraping: https://gov.texas.gov/news/post/governor-abbott-continues-state-response-recovery-efforts-for-texans-impacted-by-flooding\n",
            "ðŸ” Scraping: https://www.illinois.gov/news/press-release.30430.html\n",
            "ðŸ” Scraping: https://www.nationalguard.mil/News/Article-View/Article/3919179/national-guard-rescues-hundreds-of-people-in-wake-of-hurricane/\n",
            "âš ï¸ Error scraping https://www.nationalguard.mil/News/Article-View/Article/3919179/national-guard-rescues-hundreds-of-people-in-wake-of-hurricane/: 403 Client Error: Forbidden for url: https://www.nationalguard.mil/News/Article-View/Article/3919179/national-guard-rescues-hundreds-of-people-in-wake-of-hurricane/\n",
            "ðŸ” Scraping: https://ky.ng.mil/News/Article/4180232/national-guard-answers-the-call-as-near-historic-flooding-isolates-nelson-count/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [15/Jul/2025 15:50:44] \"POST /check/ HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš ï¸ Error scraping https://ky.ng.mil/News/Article/4180232/national-guard-answers-the-call-as-near-historic-flooding-isolates-nelson-count/: 403 Client Error: Forbidden for url: https://ky.ng.mil/News/Article/4180232/national-guard-answers-the-call-as-near-historic-flooding-isolates-nelson-count/\n",
            "ðŸ” Scraping: https://www.nationalguard.mil/News/Article-View/Article/740511/texas-guard-teams-up-with-other-agencies-to-rescue-more-than-200-from-floods/\n",
            "âš ï¸ Error scraping https://www.nationalguard.mil/News/Article-View/Article/740511/texas-guard-teams-up-with-other-agencies-to-rescue-more-than-200-from-floods/: 403 Client Error: Forbidden for url: https://www.nationalguard.mil/News/Article-View/Article/740511/texas-guard-teams-up-with-other-agencies-to-rescue-more-than-200-from-floods/\n",
            "ðŸ” Scraping: https://www.bbc.com/news/articles/cp3lpdqp7x3o\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [15/Jul/2025 15:52:18] \"POST /check/ HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ Skipping blocked domain: https://www.reddit.com/r/india/comments/1m0fyxf/as_theories_swirl_about_air_india_crash_key/\n",
            "ðŸ” Scraping: https://www.bbc.com/news/articles/cp3lpdqp7x3o\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [15/Jul/2025 15:57:58] \"POST /check/ HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Scraping: https://www.espn.com/soccer/story/_/id/45732507/chelsea-win-fifa-club-world-cup-vs-psg-cole-palmer-mvp-reaction-highlights-analysis\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [15/Jul/2025 15:59:57] \"POST /check/ HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok, conf\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from newspaper import Article\n",
        "import requests\n",
        "import numpy as np\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "conf.get_default().auth_token = os.getenv(\"NGROK_AUTH_TOKEN\")\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\" * Ngrok tunnel URL: {public_url}\")\n",
        "\n",
        "# Load the sentence transformer model\n",
        "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
        "\n",
        "# SerpAPI (Google Search) key\n",
        "SERPAPI_KEY = os.getenv(\"GOOGLE_SEARCH_API_KEY\")\n",
        "\n",
        "def search_google(query, count=3):\n",
        "    params = {\n",
        "        \"engine\": \"google\",\n",
        "        \"q\": query,\n",
        "        \"num\": count,\n",
        "        \"api_key\": SERPAPI_KEY\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(\"https://serpapi.com/search\", params=params)\n",
        "        results = response.json()\n",
        "        urls = [item[\"link\"] for item in results.get(\"organic_results\", [])]\n",
        "        return urls\n",
        "    except Exception as e:\n",
        "        print(\"Google search error:\", e)\n",
        "        return []\n",
        "\n",
        "BLOCKED_DOMAINS = [\n",
        "    \"forbes.com\", \"sciencedirect.com\", \"reddit.com\",\n",
        "    \"researchgate.net\", \"ieee.org\", \"springer.com\",\n",
        "    \"facebook.com\", \"linkedin.com\", \"twitter.com\",\n",
        "    \"instagram.com\", \"pinterest.com\", \"medium.com\",\n",
        "    \"quora.com\", \"bloomberg.com\", \"wired.com\",\n",
        "    \"nytimes.com\", \"washingtonpost.com\"\n",
        "]\n",
        "\n",
        "def is_allowed(url):\n",
        "    return not any(domain in url for domain in BLOCKED_DOMAINS)\n",
        "\n",
        "def extract_text(url):\n",
        "    if not is_allowed(url):\n",
        "        print(f\"âŒ Skipping blocked domain: {url}\")\n",
        "        return \"\"\n",
        "    try:\n",
        "        print(f\"ðŸ” Scraping: {url}\")\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        for tag in soup([\"script\", \"style\", \"footer\", \"nav\", \"noscript\", \"header\", \"meta\"]):\n",
        "            tag.decompose()\n",
        "\n",
        "        text = soup.get_text(separator=\" \")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Error scraping {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def split_sentences(text):\n",
        "    # Remove extra whitespace, HTML leftovers, etc.\n",
        "    cleaned = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Basic sentence splitter (you can improve with NLP later)\n",
        "    sentences = re.split(r'[.!?]', cleaned)\n",
        "\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "@app.route(\"/check/\", methods=[\"GET\"])\n",
        "def check_route():\n",
        "  return \"Hello World\"\n",
        "\n",
        "@app.route(\"/check/\", methods=[\"POST\"])\n",
        "def check_plagiarism():\n",
        "    data = request.get_json()\n",
        "    student_text = data.get(\"text\", \"\")\n",
        "    if not student_text:\n",
        "        return jsonify({\"error\": \"No text provided\"}), 400\n",
        "\n",
        "    student_sentences = split_sentences(student_text)\n",
        "    student_embeddings = model.encode(student_sentences, convert_to_tensor=True)\n",
        "\n",
        "    all_matches = []\n",
        "\n",
        "    for sentence in student_sentences:\n",
        "        urls = search_google(sentence)\n",
        "        for url in urls:\n",
        "            ref_text = extract_text(url)\n",
        "            ref_sentences = split_sentences(ref_text)\n",
        "            if not ref_sentences:\n",
        "                continue\n",
        "\n",
        "            ref_embeddings = model.encode(ref_sentences, convert_to_tensor=True)\n",
        "\n",
        "            for i, s_emb in enumerate(student_embeddings):\n",
        "                sim_scores = util.cos_sim(s_emb, ref_embeddings)[0]\n",
        "                best_idx = int(np.argmax(sim_scores))\n",
        "                best_score = float(sim_scores[best_idx])\n",
        "\n",
        "                if best_score > 0.75:\n",
        "                    all_matches.append({\n",
        "                        \"student_sentence\": student_sentences[i],\n",
        "                        \"matched_sentence\": ref_sentences[best_idx],\n",
        "                        \"similarity\": round(best_score, 2),\n",
        "                        \"source_url\": url\n",
        "                    })\n",
        "\n",
        "    return jsonify({\"matches\": all_matches})\n",
        "\n",
        "app.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
